{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wn2A1hcKvZz-"
   },
   "outputs": [],
   "source": [
    "# Q: how do i feed my model all the data points in the training set\n",
    "# A: Mini-Batch SGD ?\n",
    "\n",
    "# Q: What is the train set, dev set and test set ?\n",
    "\n",
    "# Q: Read up about Model Smoothing / Regularization\n",
    "\n",
    "# Q: Read up about Cross Entropy Loss\n",
    "\n",
    "# Q: Initialising of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "lXciAp52nDdx"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline\n",
    "NUM_CLASSES = 27*28\n",
    "T = torch.tensor\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "889p7eGVTgVX"
   },
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/karpathy/makemore/master/names.txt'\n",
    "df1 = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "B4-E-ugcT2fT"
   },
   "outputs": [],
   "source": [
    "words = ['emma']\n",
    "for word in df1['emma']:\n",
    "  words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09939749633190773\n"
     ]
    }
   ],
   "source": [
    "trainingDataSet = []\n",
    "devDataSet = []\n",
    "evalDataSet = []\n",
    "ind = 0\n",
    "\n",
    "p = T([0.8, 0.1, 0.1])\n",
    "multiSamples = multiSamples = torch.multinomial(p, num_samples=len(words), replacement=True, generator=g)\n",
    "# print(multiSamples)\n",
    "\n",
    "for i in multiSamples:\n",
    "    if i == 0: trainingDataSet.append(words[ind])\n",
    "    elif i == 1: devDataSet.append(words[ind])\n",
    "    else: evalDataSet.append(words[ind])\n",
    "    ind += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OZoUxsZkwsmf",
    "outputId": "00be6770-470c-489d-d5cd-3d5090a2f2ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<.e> m\n",
      "<em> m\n",
      "<mm> a\n",
      "<ma> .\n"
     ]
    }
   ],
   "source": [
    "for word in words[:1]:\n",
    "  word = '.' + word + '.'\n",
    "  for ch1, ch2, ch3 in zip(word, word[1:], word[2:]):\n",
    "    print(f\"<{ch1}{ch2}> {ch3}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SuELZazKT9rP",
    "outputId": "b2ad60ac-e33c-4c01-d979-a87892b9f939"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "756 ['.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '..', '.a', '.b', '.c', '.d', '.e', '.f', '.g', '.h', '.i', '.j', '.k', '.l', '.m', '.n', '.o', '.p', '.q', '.r', '.s', '.t', '.u', '.v', '.w', '.x', '.y', '.z', 'a.', 'aa', 'ab', 'ac', 'ad', 'ae', 'af', 'ag', 'ah', 'ai', 'aj', 'ak', 'al', 'am', 'an', 'ao', 'ap', 'aq', 'ar', 'as', 'at', 'au', 'av', 'aw', 'ax', 'ay', 'az', 'b.', 'ba', 'bb', 'bc', 'bd', 'be', 'bf', 'bg', 'bh', 'bi', 'bj', 'bk', 'bl', 'bm', 'bn', 'bo', 'bp', 'bq', 'br', 'bs', 'bt', 'bu', 'bv', 'bw', 'bx', 'by', 'bz', 'c.', 'ca', 'cb', 'cc', 'cd', 'ce', 'cf', 'cg', 'ch', 'ci', 'cj', 'ck', 'cl', 'cm', 'cn', 'co', 'cp', 'cq', 'cr', 'cs', 'ct', 'cu', 'cv', 'cw', 'cx', 'cy', 'cz', 'd.', 'da', 'db', 'dc', 'dd', 'de', 'df', 'dg', 'dh', 'di', 'dj', 'dk', 'dl', 'dm', 'dn', 'do', 'dp', 'dq', 'dr', 'ds', 'dt', 'du', 'dv', 'dw', 'dx', 'dy', 'dz', 'e.', 'ea', 'eb', 'ec', 'ed', 'ee', 'ef', 'eg', 'eh', 'ei', 'ej', 'ek', 'el', 'em', 'en', 'eo', 'ep', 'eq', 'er', 'es', 'et', 'eu', 'ev', 'ew', 'ex', 'ey', 'ez', 'f.', 'fa', 'fb', 'fc', 'fd', 'fe', 'ff', 'fg', 'fh', 'fi', 'fj', 'fk', 'fl', 'fm', 'fn', 'fo', 'fp', 'fq', 'fr', 'fs', 'ft', 'fu', 'fv', 'fw', 'fx', 'fy', 'fz', 'g.', 'ga', 'gb', 'gc', 'gd', 'ge', 'gf', 'gg', 'gh', 'gi', 'gj', 'gk', 'gl', 'gm', 'gn', 'go', 'gp', 'gq', 'gr', 'gs', 'gt', 'gu', 'gv', 'gw', 'gx', 'gy', 'gz', 'h.', 'ha', 'hb', 'hc', 'hd', 'he', 'hf', 'hg', 'hh', 'hi', 'hj', 'hk', 'hl', 'hm', 'hn', 'ho', 'hp', 'hq', 'hr', 'hs', 'ht', 'hu', 'hv', 'hw', 'hx', 'hy', 'hz', 'i.', 'ia', 'ib', 'ic', 'id', 'ie', 'if', 'ig', 'ih', 'ii', 'ij', 'ik', 'il', 'im', 'in', 'io', 'ip', 'iq', 'ir', 'is', 'it', 'iu', 'iv', 'iw', 'ix', 'iy', 'iz', 'j.', 'ja', 'jb', 'jc', 'jd', 'je', 'jf', 'jg', 'jh', 'ji', 'jj', 'jk', 'jl', 'jm', 'jn', 'jo', 'jp', 'jq', 'jr', 'js', 'jt', 'ju', 'jv', 'jw', 'jx', 'jy', 'jz', 'k.', 'ka', 'kb', 'kc', 'kd', 'ke', 'kf', 'kg', 'kh', 'ki', 'kj', 'kk', 'kl', 'km', 'kn', 'ko', 'kp', 'kq', 'kr', 'ks', 'kt', 'ku', 'kv', 'kw', 'kx', 'ky', 'kz', 'l.', 'la', 'lb', 'lc', 'ld', 'le', 'lf', 'lg', 'lh', 'li', 'lj', 'lk', 'll', 'lm', 'ln', 'lo', 'lp', 'lq', 'lr', 'ls', 'lt', 'lu', 'lv', 'lw', 'lx', 'ly', 'lz', 'm.', 'ma', 'mb', 'mc', 'md', 'me', 'mf', 'mg', 'mh', 'mi', 'mj', 'mk', 'ml', 'mm', 'mn', 'mo', 'mp', 'mq', 'mr', 'ms', 'mt', 'mu', 'mv', 'mw', 'mx', 'my', 'mz', 'n.', 'na', 'nb', 'nc', 'nd', 'ne', 'nf', 'ng', 'nh', 'ni', 'nj', 'nk', 'nl', 'nm', 'nn', 'no', 'np', 'nq', 'nr', 'ns', 'nt', 'nu', 'nv', 'nw', 'nx', 'ny', 'nz', 'o.', 'oa', 'ob', 'oc', 'od', 'oe', 'of', 'og', 'oh', 'oi', 'oj', 'ok', 'ol', 'om', 'on', 'oo', 'op', 'oq', 'or', 'os', 'ot', 'ou', 'ov', 'ow', 'ox', 'oy', 'oz', 'p.', 'pa', 'pb', 'pc', 'pd', 'pe', 'pf', 'pg', 'ph', 'pi', 'pj', 'pk', 'pl', 'pm', 'pn', 'po', 'pp', 'pq', 'pr', 'ps', 'pt', 'pu', 'pv', 'pw', 'px', 'py', 'pz', 'q.', 'qa', 'qb', 'qc', 'qd', 'qe', 'qf', 'qg', 'qh', 'qi', 'qj', 'qk', 'ql', 'qm', 'qn', 'qo', 'qp', 'qq', 'qr', 'qs', 'qt', 'qu', 'qv', 'qw', 'qx', 'qy', 'qz', 'r.', 'ra', 'rb', 'rc', 'rd', 're', 'rf', 'rg', 'rh', 'ri', 'rj', 'rk', 'rl', 'rm', 'rn', 'ro', 'rp', 'rq', 'rr', 'rs', 'rt', 'ru', 'rv', 'rw', 'rx', 'ry', 'rz', 's.', 'sa', 'sb', 'sc', 'sd', 'se', 'sf', 'sg', 'sh', 'si', 'sj', 'sk', 'sl', 'sm', 'sn', 'so', 'sp', 'sq', 'sr', 'ss', 'st', 'su', 'sv', 'sw', 'sx', 'sy', 'sz', 't.', 'ta', 'tb', 'tc', 'td', 'te', 'tf', 'tg', 'th', 'ti', 'tj', 'tk', 'tl', 'tm', 'tn', 'to', 'tp', 'tq', 'tr', 'ts', 'tt', 'tu', 'tv', 'tw', 'tx', 'ty', 'tz', 'u.', 'ua', 'ub', 'uc', 'ud', 'ue', 'uf', 'ug', 'uh', 'ui', 'uj', 'uk', 'ul', 'um', 'un', 'uo', 'up', 'uq', 'ur', 'us', 'ut', 'uu', 'uv', 'uw', 'ux', 'uy', 'uz', 'v.', 'va', 'vb', 'vc', 'vd', 've', 'vf', 'vg', 'vh', 'vi', 'vj', 'vk', 'vl', 'vm', 'vn', 'vo', 'vp', 'vq', 'vr', 'vs', 'vt', 'vu', 'vv', 'vw', 'vx', 'vy', 'vz', 'w.', 'wa', 'wb', 'wc', 'wd', 'we', 'wf', 'wg', 'wh', 'wi', 'wj', 'wk', 'wl', 'wm', 'wn', 'wo', 'wp', 'wq', 'wr', 'ws', 'wt', 'wu', 'wv', 'ww', 'wx', 'wy', 'wz', 'x.', 'xa', 'xb', 'xc', 'xd', 'xe', 'xf', 'xg', 'xh', 'xi', 'xj', 'xk', 'xl', 'xm', 'xn', 'xo', 'xp', 'xq', 'xr', 'xs', 'xt', 'xu', 'xv', 'xw', 'xx', 'xy', 'xz', 'y.', 'ya', 'yb', 'yc', 'yd', 'ye', 'yf', 'yg', 'yh', 'yi', 'yj', 'yk', 'yl', 'ym', 'yn', 'yo', 'yp', 'yq', 'yr', 'ys', 'yt', 'yu', 'yv', 'yw', 'yx', 'yy', 'yz', 'z.', 'za', 'zb', 'zc', 'zd', 'ze', 'zf', 'zg', 'zh', 'zi', 'zj', 'zk', 'zl', 'zm', 'zn', 'zo', 'zp', 'zq', 'zr', 'zs', 'zt', 'zu', 'zv', 'zw', 'zx', 'zy', 'zz']\n",
      "756\n"
     ]
    }
   ],
   "source": [
    "chars = ['.'] + list(map(chr, range(97, 123)))\n",
    "doubleChars = []\n",
    "\n",
    "for ch1 in chars:\n",
    "  for ch2 in chars:\n",
    "    doubleChars.append(ch1 + ch2)\n",
    "\n",
    "totalChars = chars + doubleChars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "vMRd5eILywoU"
   },
   "outputs": [],
   "source": [
    "stoi = {chr: i for i, chr in enumerate(totalChars)} \n",
    "\n",
    "itos = {i: chr for chr, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8K_rKJQhzy2q",
    "outputId": "3fa84891-4b18-4733-a03f-351e30859d43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 32, 175, 391,  ..., 360, 271,  62]) 6000\n",
      "tensor([13, 13,  1,  ...,  1,  8,  0]) 6000\n"
     ]
    }
   ],
   "source": [
    "inputs = []\n",
    "labels = []\n",
    "trainingSetSize = 0\n",
    "for word in trainingDataSet:  \n",
    "  word = '.' + word + '.'\n",
    "  for ch1, ch2, ch3 in zip(word, word[1:], word[2:]):\n",
    "    # print(f\"<{ch1}{ch2}> {ch3}\")\n",
    "\n",
    "    y = stoi[ch3]\n",
    "    x = stoi[ch1 + ch2] \n",
    "\n",
    "    inputs.append(x)\n",
    "    labels.append(y)\n",
    "    trainingSetSize += 1\n",
    "    \n",
    "inputs = T(inputs)\n",
    "labels = T(labels)\n",
    "print(inputs, len(inputs))\n",
    "print(labels, len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hrWn4ZgSVtEs",
    "outputId": "6b3f8d16-8d7a-402e-b7dd-6d3033bb2143"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Size: 6000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training Set Size: {trainingSetSize}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Uwu9magJ8hMA"
   },
   "outputs": [],
   "source": [
    "# One-Hot Encoding\n",
    "xenc = F.one_hot(inputs, num_classes=NUM_CLASSES).float()\n",
    "\n",
    "# Building the NN -> (Weight Matrix)\n",
    "W = torch.randn((NUM_CLASSES, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "FrTf8otN_NDu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative log likelihood (loss): 1.5692129135131836\n",
      "negative log likelihood (loss): 1.5691012144088745\n",
      "negative log likelihood (loss): 1.5689901113510132\n",
      "negative log likelihood (loss): 1.5688796043395996\n",
      "negative log likelihood (loss): 1.5687696933746338\n",
      "negative log likelihood (loss): 1.568660855293274\n",
      "negative log likelihood (loss): 1.5685522556304932\n",
      "negative log likelihood (loss): 1.5684444904327393\n",
      "negative log likelihood (loss): 1.5683374404907227\n",
      "negative log likelihood (loss): 1.5682311058044434\n",
      "negative log likelihood (loss): 1.5681251287460327\n",
      "negative log likelihood (loss): 1.568019986152649\n",
      "negative log likelihood (loss): 1.5679155588150024\n",
      "negative log likelihood (loss): 1.5678114891052246\n",
      "negative log likelihood (loss): 1.5677083730697632\n",
      "negative log likelihood (loss): 1.5676056146621704\n",
      "negative log likelihood (loss): 1.567503571510315\n",
      "negative log likelihood (loss): 1.5674021244049072\n",
      "negative log likelihood (loss): 1.5673013925552368\n",
      "negative log likelihood (loss): 1.567201018333435\n",
      "negative log likelihood (loss): 1.5671013593673706\n",
      "negative log likelihood (loss): 1.567002296447754\n",
      "negative log likelihood (loss): 1.5669039487838745\n",
      "negative log likelihood (loss): 1.5668059587478638\n",
      "negative log likelihood (loss): 1.5667086839675903\n",
      "negative log likelihood (loss): 1.5666117668151855\n",
      "negative log likelihood (loss): 1.5665156841278076\n",
      "negative log likelihood (loss): 1.5664201974868774\n",
      "negative log likelihood (loss): 1.5663248300552368\n",
      "negative log likelihood (loss): 1.5662304162979126\n",
      "negative log likelihood (loss): 1.566136360168457\n",
      "negative log likelihood (loss): 1.5660427808761597\n",
      "negative log likelihood (loss): 1.5659499168395996\n",
      "negative log likelihood (loss): 1.5658574104309082\n",
      "negative log likelihood (loss): 1.565765619277954\n",
      "negative log likelihood (loss): 1.5656743049621582\n",
      "negative log likelihood (loss): 1.565583348274231\n",
      "negative log likelihood (loss): 1.5654929876327515\n",
      "negative log likelihood (loss): 1.5654033422470093\n",
      "negative log likelihood (loss): 1.5653139352798462\n",
      "negative log likelihood (loss): 1.5652251243591309\n",
      "negative log likelihood (loss): 1.5651369094848633\n",
      "negative log likelihood (loss): 1.5650489330291748\n",
      "negative log likelihood (loss): 1.564961552619934\n",
      "negative log likelihood (loss): 1.5648746490478516\n",
      "negative log likelihood (loss): 1.5647884607315063\n",
      "negative log likelihood (loss): 1.5647026300430298\n",
      "negative log likelihood (loss): 1.5646170377731323\n",
      "negative log likelihood (loss): 1.564531922340393\n",
      "negative log likelihood (loss): 1.5644477605819702\n",
      "negative log likelihood (loss): 1.5643635988235474\n",
      "negative log likelihood (loss): 1.5642799139022827\n",
      "negative log likelihood (loss): 1.5641969442367554\n",
      "negative log likelihood (loss): 1.5641140937805176\n",
      "negative log likelihood (loss): 1.564031958580017\n",
      "negative log likelihood (loss): 1.5639501810073853\n",
      "negative log likelihood (loss): 1.563868761062622\n",
      "negative log likelihood (loss): 1.5637879371643066\n",
      "negative log likelihood (loss): 1.5637073516845703\n",
      "negative log likelihood (loss): 1.5636274814605713\n",
      "negative log likelihood (loss): 1.5635478496551514\n",
      "negative log likelihood (loss): 1.5634685754776\n",
      "negative log likelihood (loss): 1.5633900165557861\n",
      "negative log likelihood (loss): 1.5633115768432617\n",
      "negative log likelihood (loss): 1.563233733177185\n",
      "negative log likelihood (loss): 1.5631558895111084\n",
      "negative log likelihood (loss): 1.563078761100769\n",
      "negative log likelihood (loss): 1.5630022287368774\n",
      "negative log likelihood (loss): 1.5629258155822754\n",
      "negative log likelihood (loss): 1.5628498792648315\n",
      "negative log likelihood (loss): 1.562774419784546\n",
      "negative log likelihood (loss): 1.5626991987228394\n",
      "negative log likelihood (loss): 1.5626243352890015\n",
      "negative log likelihood (loss): 1.5625501871109009\n",
      "negative log likelihood (loss): 1.5624762773513794\n",
      "negative log likelihood (loss): 1.562402367591858\n",
      "negative log likelihood (loss): 1.5623290538787842\n",
      "negative log likelihood (loss): 1.5622564554214478\n",
      "negative log likelihood (loss): 1.5621839761734009\n",
      "negative log likelihood (loss): 1.5621118545532227\n",
      "negative log likelihood (loss): 1.562040090560913\n",
      "negative log likelihood (loss): 1.5619688034057617\n",
      "negative log likelihood (loss): 1.5618977546691895\n",
      "negative log likelihood (loss): 1.5618268251419067\n",
      "negative log likelihood (loss): 1.5617568492889404\n",
      "negative log likelihood (loss): 1.5616868734359741\n",
      "negative log likelihood (loss): 1.561617136001587\n",
      "negative log likelihood (loss): 1.561547875404358\n",
      "negative log likelihood (loss): 1.5614789724349976\n",
      "negative log likelihood (loss): 1.5614103078842163\n",
      "negative log likelihood (loss): 1.5613421201705933\n",
      "negative log likelihood (loss): 1.5612742900848389\n",
      "negative log likelihood (loss): 1.5612066984176636\n",
      "negative log likelihood (loss): 1.5611393451690674\n",
      "negative log likelihood (loss): 1.5610724687576294\n",
      "negative log likelihood (loss): 1.5610058307647705\n",
      "negative log likelihood (loss): 1.5609397888183594\n",
      "negative log likelihood (loss): 1.5608737468719482\n",
      "negative log likelihood (loss): 1.5608080625534058\n",
      "negative log likelihood (loss): 1.5607426166534424\n"
     ]
    }
   ],
   "source": [
    "stepSize = 100\n",
    "for iterations in range(100):\n",
    "  # Forward-Pass\n",
    "\n",
    "  regularization = 0\n",
    "  logits = W[inputs, :] # logits -> log-counts\n",
    "\n",
    "  # Matrix Multiplication \n",
    "  # logits = xenc @ W \n",
    "  counts = logits.exp() # getting Counts\n",
    "\n",
    "  # Normalizing -> converting counts into a Probability Distribution\n",
    "  P = counts / counts.sum(1, keepdims=True)\n",
    "\n",
    "  # Finding Loss -> product of probablities of observing the data, converted into negative logs \n",
    "  nlls = torch.zeros(trainingSetSize)\n",
    "  for result_i in range(trainingSetSize):\n",
    "#     x, y = inputs[i].item(), labels[i].items()\n",
    "    \n",
    "    label_i = labels[result_i].item()\n",
    "    likelihood = P[result_i, label_i]\n",
    "    nll = -torch.log(likelihood)\n",
    "\n",
    "    nlls[result_i] = nll\n",
    "\n",
    "  loss = nlls.mean() + regularization\n",
    "\n",
    "  # Backward-Pass\n",
    "  W.grad = None\n",
    "  loss.backward()\n",
    "\n",
    "  # Gradient Descent\n",
    "  W.data -= stepSize * W.grad\n",
    "\n",
    "  print(f\"negative log likelihood (loss): {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "J_8Noq9nYDJ9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "randrie\n",
      "wila\n",
      "phitson\n",
      "oan\n",
      "on\n",
      "paisaislortumlsuppvmvqde\n",
      "ymaraxqcssielanah\n",
      "gainnia\n",
      "\n",
      "oana\n",
      "marlee\n",
      "oanina\n",
      "olqfdwhoajzccdlee\n",
      "zacie\n",
      "gani\n",
      "zsailla\n",
      "za\n",
      "zophhocpdgo\n",
      "patalee\n",
      "zakcdkf\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "  out = []\n",
    "  ix1 = None\n",
    "  ix2 = 0\n",
    "\n",
    "  while True:\n",
    "\n",
    "    if ix1 == None:\n",
    "      ix = ix2\n",
    "    else:\n",
    "      ix = stoi[itos[ix1] + itos[ix2]]\n",
    "\n",
    "    logits = W[ix].clone().detach()\n",
    "    count = logits.exp()\n",
    "    probDist = count / count.sum()\n",
    "    ix3 = torch.multinomial(probDist, num_samples=1, replacement=True, generator=g).item()\n",
    "    \n",
    "#     if ix1 != None: print(f\"<{itos[ix1]}{itos[ix2]}> - {itos[ix3]}\")\n",
    "#     else: print(f\"<{itos[ix2]}> - {itos[ix3]}\")\n",
    "\n",
    "    if ix3 == 0:\n",
    "      break\n",
    "    out.append(itos[ix3])\n",
    "    ix1, ix2 = ix2, ix3\n",
    "    \n",
    "  print(''.join(out))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
